#include "cumsum_plugin.h"

using namespace nvinfer1;

namespace trtorch {
namespace core {
namespace conversion {
namespace converters {
namespace impl {
namespace plugins {

/*
 * CumsumPlugin class implementations
 */

CumsumPlugin::CumsumPlugin(int dim) : dim_(dim) {}

CumsumPlugin::CumsumPlugin(const char* data, size_t length) {
  std::istringstream data_stream(std::string(data, length));

  torch::serialize::InputArchive input_archive;
  input_archive.load_from(data_stream);

  {
    torch::IValue value;
    input_archive.read("dim", value);

    dim_ = value.toInt();
  }
}

int CumsumPlugin::getNbOutputs() const {
  return 1;
}

const char* CumsumPlugin::getPluginType() const {
  return "Cumsum";
}

const char* CumsumPlugin::getPluginVersion() const {
  return "1";
}

const char* CumsumPlugin::getPluginNamespace() const {
  return "";
}

nvinfer1::IPluginV2DynamicExt* CumsumPlugin::clone() const {
  return new CumsumPlugin(dim_);
}

nvinfer1::DimsExprs CumsumPlugin::getOutputDimensions(
    int outputIndex,
    const nvinfer1::DimsExprs* inputs,
    int nbInputs,
    nvinfer1::IExprBuilder& exprBuilder) {
  return inputs[0];
}

nvinfer1::DataType CumsumPlugin::getOutputDataType(int index, const nvinfer1::DataType* inputTypes, int nbInputs)
    const {
  return inputTypes[index];
}

int CumsumPlugin::initialize() {
  return 0;
}

void CumsumPlugin::serialize(void* buffer) const {
  std::string data = serializeToString();
  size_t size = getSerializationSize();

  data.copy((char*)buffer, size);
}

std::string CumsumPlugin::serializeToString() const {
  torch::serialize::OutputArchive output_archive;

  output_archive.write("dim", torch::IValue(dim_));

  std::ostringstream data_str;
  output_archive.save_to(data_str);

  return data_str.str();
}

size_t CumsumPlugin::getSerializationSize() const {
  return serializeToString().size();
}

bool CumsumPlugin::supportsFormatCombination(
    int pos,
    const nvinfer1::PluginTensorDesc* inOut,
    int nbInputs,
    int nbOutputs) {
  TRTORCH_ASSERT(0 <= pos && pos <= 1, "There should be exactly 2 connections to the plugin - 1 input, 1 output");
  TRTORCH_ASSERT(nbInputs == 1, "Expected a single tensor as input to cumsum plugin");
  TRTORCH_ASSERT(nbOutputs == 1, "Expected a single tensor as output to cumsum plugin");

  const PluginTensorDesc& in = inOut[0];

  if (pos == 0) {
    return (in.type == nvinfer1::DataType::kFLOAT || in.type == nvinfer1::DataType::kINT32 || in.type == nvinfer1::DataType::kHALF) &&
        (in.format == nvinfer1::TensorFormat::kLINEAR);
  }

  // pos == 1, accessing information about output tensor
  const PluginTensorDesc& out = inOut[1];

  return (in.type == out.type) && (in.format == out.format);
}

void CumsumPlugin::configurePlugin(
    const nvinfer1::DynamicPluginTensorDesc* in,
    int nbInputs,
    const nvinfer1::DynamicPluginTensorDesc* out,
    int nbOutputs) {}

size_t CumsumPlugin::getWorkspaceSize(
    const nvinfer1::PluginTensorDesc* inputs,
    int nbInputs,
    const nvinfer1::PluginTensorDesc* outputs,
    int nbOutputs) const {
  return 0;
}

int CumsumPlugin::enqueue(
    const nvinfer1::PluginTensorDesc* inputDesc,
    const nvinfer1::PluginTensorDesc* outputDesc,
    const void* const* inputs,
    void* const* outputs,
    void* workspace,
    cudaStream_t stream) {
  auto tensor_type = util::toATenDType(inputDesc[0].type);
  at::Tensor input = at::from_blob((void*)inputs[0], util::toVec(inputDesc[0].dims), [](void*) {}, at::device(at::kCUDA).dtype(tensor_type));
  at::Tensor output = at::from_blob(
      outputs[0], util::toVec(outputDesc->dims), [](void*) {}, at::device(at::kCUDA).dtype(tensor_type));

  at::cuda::CUDAStream torch_stream = at::cuda::getStreamFromPool();
  at::cuda::CUDAStreamGuard torch_guard(torch_stream);

  cudaEvent_t event;
  cudaEventCreate(&event);
  cudaEventRecord(event, stream);

  cudaStreamWaitEvent(torch_stream.stream(), event, 0);

  at::cumsum_out(output, input, dim_);

  cudaEvent_t torch_event;
  cudaEventCreate(&torch_event);
  cudaEventRecord(torch_event, torch_stream.stream());

  cudaStreamWaitEvent(stream, torch_event, 0);

  cudaEventDestroy(event);
  cudaEventDestroy(torch_event);

  return 0;
}

/*
 * CumsumPluginCreator class implementations
 */
const char* CumsumPluginCreator::getPluginNamespace() const {
  return "";
}

const char* CumsumPluginCreator::getPluginName() const {
  return "Cumsum";
}

const char* CumsumPluginCreator::getPluginVersion() const {
  return "1";
}

nvinfer1::IPluginV2* CumsumPluginCreator::createPlugin(const char* name, const nvinfer1::PluginFieldCollection* fc) {
  return nullptr;
}

CumsumPlugin* CumsumPluginCreator::createPlugin(const char* name, int dim) {
  name_ = name;
  return new CumsumPlugin(dim);
}

nvinfer1::IPluginV2* CumsumPluginCreator::deserializePlugin(
    const char* name,
    const void* serialData,
    size_t serialLength) {
  name_ = name;
  return new CumsumPlugin((const char*)serialData, serialLength);
}

const nvinfer1::PluginFieldCollection* CumsumPluginCreator::getFieldNames() {
  return nullptr;
}

REGISTER_TENSORRT_PLUGIN(CumsumPluginCreator);

} // namespace plugins
} // namespace impl
} // namespace converters
} // namespace conversion
} // namespace core
} // namespace trtorch 